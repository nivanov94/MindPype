{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af1b209-0596-44d2-89de-32e19299edda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is to setup the path so we can import the mindpype library\n",
    "import os; os.sys.path.append(os.path.dirname(os.path.abspath('.')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c63ee0-c84a-43f7-9882-d4815578922e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mindpype\n",
    "import mindpype as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23a947d",
   "metadata": {},
   "source": [
    "We will start by importing the training and testing files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690a2224-3d0b-4dde-96c7-051725bcc199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the training and testing files\n",
    "from glob import glob\n",
    "files = glob(\"P:/general_prism/Side Projects/NIRS BCI/Data/Dec4/sub-P003/sourcedata/*.xdf\")\n",
    "print(files)\n",
    "\n",
    "# training files\n",
    "training_files = files[1:-1]\n",
    "\n",
    "# testing files (one from each task)\n",
    "testing_files = [files[0], files[-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d5bf64",
   "metadata": {},
   "source": [
    "The first step to creating a pipeline is to create a session, which serves as a sandbox for all components in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895739a3-56c6-471f-9e5a-5c200bd0ee6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the mindpype session\n",
    "s = mp.Session.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90053e00",
   "metadata": {},
   "source": [
    "Next, we will define some session parameters that will be used to create our data sources and nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e5fad1-1e47-45d3-a2e1-a05122ef9981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some session parameters\n",
    "Fs = 50\n",
    "Nc = 46\n",
    "trial_len = 15\n",
    "Ns = int(trial_len * Fs)\n",
    "\n",
    "epoch_len = int(Fs * 4)\n",
    "epoch_stride = int(Fs * 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e33f658",
   "metadata": {},
   "source": [
    "Then, we will create our training and test data sources. For our training and test data sources, we will created an epoched XDF file input source using the ```create_epoched()``` factory method. \n",
    "\n",
    "For the training data, we will then seperate the input source data into a tensor (containing stream data) and a label tensor (containing marker data) by using the ```load_into_tensors()``` method.\n",
    "\n",
    "For our test data, we will create a volatile tensor to ingest data from the epoched XDF file input source using the ```create_from_handle()``` factory method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27ae807-bd29-49d7-90de-ffafa3755d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the data sources\n",
    "\n",
    "# training data\n",
    "tr_data_src = mp.source.InputXDFFile.create_epoched(s, training_files, channels=range(Nc),\n",
    "                                                    tasks=['{\"status\": \"Neutral\"}', '{\"status\": \"Music\"}'],\n",
    "                                                    stype='NIRS', Ns=Ns)\n",
    "t_tr_data, t_tr_labels = tr_data_src.load_into_tensors()\n",
    "\n",
    "# test data\n",
    "te_data_src = mp.source.InputXDFFile.create_epoched(s, testing_files, channels=range(Nc),\n",
    "                                                    tasks=['{\"status\": \"Neutral\"}', '{\"status\": \"Music\"}'],\n",
    "                                                    stype='NIRS', Ns=Ns)\n",
    "\n",
    "# create a volatile data edge to ingest data from the source\n",
    "t_data_in = mp.Tensor.create_from_handle(s, (Nc, Ns), te_data_src)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720da0fe",
   "metadata": {},
   "source": [
    "We will also create a tensor to to store the prediction that is outputed by the classifier in our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06425d72-fc9f-467d-a840-931399ea43df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the edge that will store the classifier output\n",
    "t_pred = mp.Tensor.create(s, (1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6110702f",
   "metadata": {},
   "source": [
    "Next, we will create virtual tensors to store all of our intermediate data calculated in our pipeline by using the ```create_virtual()``` factory method. Since these edges represent intermediate data that is only required in the process of completing a calculation and we don't need to access them at a leter point we use the virtual type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521b2206-fc10-4808-bba1-a82050da6923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our virtual edges\n",
    "\n",
    "v_tensors = [\n",
    "                mp.Tensor.create_virtual(s),  #  0 - filtered data\n",
    "                mp.Tensor.create_virtual(s),  #  1 - epoched data\n",
    "                mp.Tensor.create_virtual(s),  #  2 - mean\n",
    "                mp.Tensor.create_virtual(s),  #  3 - variance\n",
    "                mp.Tensor.create_virtual(s),  #  4 - kurtosis\n",
    "                mp.Tensor.create_virtual(s),  #  5 - skew\n",
    "                mp.Tensor.create_virtual(s),  #  6 - slope\n",
    "                mp.Tensor.create_virtual(s),  #  7 - mean+var\n",
    "                mp.Tensor.create_virtual(s),  #  8 - mean+var+kurt\n",
    "                mp.Tensor.create_virtual(s),  #  9 - mean+var+kurt+skew\n",
    "                mp.Tensor.create_virtual(s),  # 10 - mean+var+kurt+skew+slope\n",
    "                mp.Tensor.create_virtual(s),  # 11 - flattened feature vector\n",
    "                mp.Tensor.create_virtual(s),  # 12 - normalized feature vector\n",
    "                mp.Tensor.create_virtual(s),  # 13 - selected features\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fd4a26",
   "metadata": {},
   "source": [
    "Next, we will create objects that will be used as parameters for the various nodes in our graph. This includes a filter object that we will pass to our filtering node, and an Classifier object that we will pass to our classification node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc317838-f108-4a1f-b625-73f356e69803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create filter and classifier objects\n",
    "# these objects will be parameters to the filter and classifier nodes in the graph\n",
    "\n",
    "# 4th order Butterworth filter with passband of 0.1-8 H\n",
    "mp_filt = mp.Filter.create_butter(s, 4, (0.1, 8), 'bandpass', 'sos', Fs)\n",
    "\n",
    "# LDA classifier\n",
    "mp_clsf = mp.Classifier.create_LDA(s, shrinkage='auto', solver='lsqr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1b3d98",
   "metadata": {},
   "source": [
    "Now, we will create our graph and add our various nodes to the graph using the ```add_to_graph()``` factory method. \n",
    "\n",
    "For our fNirs pipeline, we will use nodes to filter our data, epoch the data, compute and concatenate features, flatten/normalize/select features, and classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6337cd8e-69dd-48bb-ac76-650f97824966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the graph and add nodes\n",
    "g = mp.Graph.create(s)\n",
    "\n",
    "# filter the data\n",
    "mp.kernels.FiltFiltKernel.add_to_graph(g, t_data_in, mp_filt, v_tensors[0],\n",
    "                                       init_input=t_tr_data,  # Note inserting the training data here so that it goes through the entire graph\n",
    "                                       init_labels=t_tr_labels)\n",
    "\n",
    "# epoch the data\n",
    "mp.kernels.EpochKernel.add_to_graph(g, v_tensors[0], v_tensors[1],\n",
    "                                    epoch_len=epoch_len,\n",
    "                                    epoch_stride=epoch_stride,\n",
    "                                    axis=1)\n",
    "\n",
    "# compute features\n",
    "mp.kernels.MeanKernel.add_to_graph(g, v_tensors[1], v_tensors[2],\n",
    "                                   axis=2, keepdims=True)\n",
    "\n",
    "mp.kernels.VarKernel.add_to_graph(g, v_tensors[1], v_tensors[3],\n",
    "                                  axis=2, keepdims=True)\n",
    "\n",
    "mp.kernels.KurtosisKernel.add_to_graph(g, v_tensors[1], v_tensors[4],\n",
    "                                       axis=2, keepdims=True)\n",
    "\n",
    "mp.kernels.SkewnessKernel.add_to_graph(g, v_tensors[1], v_tensors[5],\n",
    "                                       axis=2, keepdims=True)\n",
    "\n",
    "mp.kernels.SlopeKernel.add_to_graph(g, v_tensors[1], v_tensors[6], Fs=Fs)\n",
    "\n",
    "# concatenation the features\n",
    "mp.kernels.ConcatenationKernel.add_to_graph(g, v_tensors[2], v_tensors[3], v_tensors[7], axis=2)\n",
    "mp.kernels.ConcatenationKernel.add_to_graph(g, v_tensors[7], v_tensors[4], v_tensors[8], axis=2)\n",
    "mp.kernels.ConcatenationKernel.add_to_graph(g, v_tensors[8], v_tensors[5], v_tensors[9], axis=2)\n",
    "mp.kernels.ConcatenationKernel.add_to_graph(g, v_tensors[9], v_tensors[6], v_tensors[10], axis=2)\n",
    "\n",
    "# flatten the feature vector\n",
    "mp.kernels.ReshapeKernel.add_to_graph(g, v_tensors[10], v_tensors[11], (-1,))\n",
    "\n",
    "# normalize the features\n",
    "mp.kernels.FeatureNormalizationKernel.add_to_graph(g, v_tensors[11], v_tensors[12], axis=0)\n",
    "\n",
    "# select features\n",
    "mp.kernels.FeatureSelectionKernel.add_to_graph(g, v_tensors[12], v_tensors[13], k=100)\n",
    "\n",
    "# classify\n",
    "mp.kernels.ClassifierKernel.add_to_graph(g, v_tensors[13], mp_clsf, t_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9324c15d",
   "metadata": {},
   "source": [
    "From the input XDF source we will extract the test labels into a tensor by using the ```load_into_tensors()``` method. This method returns 2-4 tensors with the second one being the marker data, so we will use the index 1 to access these labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d966f11d-e9f5-4c75-92ec-43611283b2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the testing labels for later\n",
    "t_te_labels = te_data_src.load_into_tensors()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb019871",
   "metadata": {},
   "source": [
    "We will then verify the graph using the ```verify()``` method. Verifying the graph orders the nodes for execution and ensure that the inputs and outputs of each processing node are appropriately typed and sized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7de362-18b9-468d-a299-4a008acfd764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the graph\n",
    "g.verify()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70f970b",
   "metadata": {},
   "source": [
    "Using the prediction outputed from our classifier node, we will perform cross validation on our graph and print the accuracy.\n",
    "\n",
    "The next step is to initialize the graph using the ```initialize()``` method. This step is required for pipelines that have methods that need to be trained or fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8356d1e0-1174-442a-9b8d-d202f41d32e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validate and init graph\n",
    "cv_acc = g.cross_validate(t_pred)\n",
    "print(f\"Cross validation accuracy {cv_acc:0.3f}\")\n",
    "\n",
    "g.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d384d181",
   "metadata": {},
   "source": [
    "We are now ready to run our pipeline. To run the graph for the provided input data, we use the ```execute()``` method. Since we are using epoched/class-seperated data, the trial labels are known, so we can pass them into the execute method. We will run the graph twice, once for each test label, and print the true and predicted label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be13372-93de-4808-b7dc-f88be7fde760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the graph twice, once for each test trial\n",
    "for i_t, label in enumerate(t_te_labels.data):\n",
    "    g.execute(label=label)\n",
    "    print(f\"Trial {i_t+1} - True label: {label}, predicted label: {t_pred.data[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
