{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will demonstrate how to work with XDF objects in MindPype. We will create a pipeline that performs addition to demonstrate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is to setup the path so we can import the mindpype library\n",
    "import os; os.sys.path.append(os.path.dirname(os.path.abspath('.')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindpype as mp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = [i for i in range(3,10)]\n",
    "tasks = ('flash', 'target')\n",
    "file = ['/path/to/mindset/data']\n",
    "start = -.2\n",
    "samples = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to creating a pipeline is to create a session, which serves as a sandbox for all components in the pipeline. After creating the session we will create two graphs, one to test epoched data and one to test continous data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a session\n",
    "session = mp.Session.create()\n",
    "\n",
    "# Create two graphs, one to test the epoched data and one to test the continuous data\n",
    "graph = mp.Graph.create(session)\n",
    "graph_cont = mp.Graph.create(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create XDF objects for our epoched and continous data. This can be done by using the ```create_epoched()``` and ```create_continuous()``` factory methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the XDF objects for the epoched and continuous data using the factory methods\n",
    "xdf_object = mp.source.BcipXDF.create_epoched(session, file, tasks, channels, start, samples)\n",
    "xdf_object_continuous = mp.source.BcipXDF.create_continuous(session, file, tasks, channels, start, samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will create our input and output data containers for the graph. For both the epoched and the continuous case, we can use the factory method ```create_from_handle()``` to create a Tensor from the XDF sources.\n",
    "\n",
    "For the second input to our addition kernel we can use the ```create_from_data()``` factory method to create our Tensor since the input data is known.\n",
    "\n",
    "We will also create Tensors to store our output data for the epoched and continuous case. We can create a Tensor of a specified shape using the ```Tensor.create()``` factory method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the input tensors for the epoched and continuous data\n",
    "t_in = mp.Tensor.create_from_handle(session, (len(channels), samples), xdf_object)\n",
    "t_in_cont = mp.Tensor.create_from_handle(session, (len(channels), samples), xdf_object_continuous)\n",
    "\n",
    "# Create an input tensor for the second input to the addition kernel\n",
    "t_in_2 = mp.Tensor.create_from_data(session, data=np.ones(t_in.shape))\n",
    "\n",
    "# Create the output tensors for the epoched and continuous data\n",
    "t_out = mp.Tensor.create(session, shape=t_in.shape)\n",
    "t_out_cont = mp.Tensor.create(session, shape=t_in.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will add an addition kernel to the epoched and continous graphs by using the ```add_to_graph()``` factory method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the addition kernel to each graph\n",
    "mp.kernels.AdditionKernel.add_to_graph(graph, t_in, t_in_2, t_out)\n",
    "mp.kernels.AdditionKernel.add_to_graph(graph_cont, t_in_cont, t_in_2, t_out_cont)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have created the processing pipeline, and added all of the required components. We can now use a graph method to verify that the structure of the pipeline is valid.\n",
    "\n",
    "Following the verification of the pipeline, we should now initialize the graph. This step is required for pipelines that have methods that need to be trained or fit (ie. classifiers), but optional for other pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify and initialize the graphs\n",
    "graph.verify() \n",
    "graph_cont.verify()\n",
    "\n",
    "graph.initialize() \n",
    "graph_cont.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can run our pipeline by calling the ```execute()``` method on our graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the graphs\n",
    "for i in range(9):\n",
    "    graph.execute()\n",
    "    graph_cont.execute()  \n",
    "\n",
    "    # Check that the output tensors are equal to the sum of the input tensors\n",
    "    print(np.array_equal(t_out.data, t_in.data + t_in_2.data))\n",
    "    print(np.array_equal(t_out_cont.data, t_in_cont.data + t_in_2.data)) \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
